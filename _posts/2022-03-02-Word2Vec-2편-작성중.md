---
categories:
  - nlp

tags:
  - Embedding

layout: single

toc: true
toc_sticky: true
use_math: true
comments: true
---

---

안녕하세요, [저번 포스팅](https://kthworks.github.io/nlp/Word2Vec-1%ED%8E%B8/#cbow-countinuous-bag-of-words)에서는 [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf)에 대한 기초 개념부터 weight update까지 다루어보았는데요. 이번 포스팅에서는 Word2Vec에서 연산량을 줄이기 위한 개선 방법을 소개하려고 합니다. 이 개선 방법은 2013년 구글에서 [Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) 라는 제목으로 발표한 논문에서 다루고 있습니다.

## Background

Word2Vec은 [NPLM](https://kthworks.github.io/nlp/Neural-Probabilistic-Language-Model-(NPLM)/)의 hidden layer를 과감히 제거하여 연산량을 크게 줄였습니다. 하지만, Vocabulary 안의 단어 수가 10만개, 100만개로 늘어난다면 연산량 또한 기하급수적으로 늘어날 수 밖에 없습니다.

이를 해결하기 위해 Word2Vec에서는 2가지 방법을 사용하는데요, 바로 **계층적 소프트맥스(Hierarchical Softmax** 와 **네거티브 샘플링(Negative sampling)** 입니다.

하나씩 살펴보도록 하겠습니다.

## 계층적 소프트맥스 (Hierarchical Softmax)
Vocabulary의 단어 수가 많아졌을 때 가장 연산량을 많이 잡아먹는 부분은 Softmax부분입니다. Vocabulary가 가지고 있는 단어의 수(V)만큼의 score에 대해서 일일이 계산을 하여 총 합이 1이 되도록 만들어야 하기 때문이죠. 이를 방지하기 위해서 **Softmax 대신 binary tree를 이용한 multinomial distribution function(다항분포함수)을 사용** 합니다.

먼저, 아래와 같이 모든 단어들을 binary tree를 이용하여 표현한다고 해봅시다. tree의 각 잎(leaf, 트리의 말단)은 각 단어이며, 가장 위에 있는 노드인 뿌리(root)로부터 각 잎까지는 유일한 경로(path)로 연결됩니다.

Hierarchial Softmax를 사용할 경우 기존 CBOW나 Skip-gram에 있던 $W'$ matrix를 사용하지 않습니다. 대신, V-1개의 internal node(내부 노드)가 각각 길이 N짜리 weight vector를 가지게 됩니다. 이를 $v'_{i}$ 라고 하고 학습을 통해 update 합니다.


<center>
<img src="/images/Word2Vec/hsexample.png" width="600" height="300">
</center>

### Notations
내용을 본격적으로 설명하기 전에, 사용되는 notation들에 대해 먼저 설명 드리겠습니다.

**n(w2,2)** : 뿌리(root)부터 w2까지 가는 경로(path) 중 2번째로 만나는 노드  
**L(w2)** : 뿌리(root)부터 w2까지 가는 경로(path)의 길이    
**ch(node)** : 특정 node에서의 고정된 임의의 한 자식(child)  
**\[[x]]** : x가 true일 경우 1, false일 경우 -1을 반환하는   
**h** : projection layer의 vector  
**${v'}_{i}$** : 각 Internal node(내부 노드)가 가지는 N차원의 weight vector  
$\mathbf{\sigma(x)}$ : sigmoid function ( $ \frac{1}{1+\exp(-x)}$   




### Equation

Tree를 이용한 확률에 대한 수식을 살펴보겠습니다. 임의의 단어 $w$가 주변 단어 $w_{O}$가 될 확률은 아래와 같이 정의됩니다.

<center>
<img src="/images/Word2Vec/hsequation.png" width="600" height="100">
</center>

위의 수식에서 $\sigma$ 함수 안을 살펴 봅시다.
$$n(w, j+1) = ch(n(w,j))$$

위 식은 Tree 내의 특정 노드 (즉, 뿌리부터 단어 w까지 가는 경로에서 j+1번째 노드)가 바로 위에 있는 노드(뿌리부터 단어 w까지 가는 경로에서 j번째 노드)의 고정된 child냐는 뜻입니다.

다시 말해서, $n(w,j)$ 노드에서 왼쪽으로 가는 경우와 오른쪽으로 가는 경우에 따라 $\sigma$ 함수 안은

${v'}^{T}_{n(w,j)} h$ 또는

${-v'}^{T}_{n(w,j)} h$ 가 되는 것이죠.

${v'}_{n(w,j)}^{T} h = x$ 로 치환하면, 결국 $\sigma(x)$ 또는 $\sigma(-x)$ 가 됩니다.

특정 노드에서 왼쪽으로 가거나 오른쪽으로 가는 두 가지 선택지의 합을 나타내면 $\sigma(x) + \sigma(-x)$인데요, Sigmoid 함수는 $\sigma(x) + \sigma(-x) = 1$ 을 만족하는 특징이 있습니다.

따라서 Tree 안의 모든 node가 이 관계를 만족하게 되므로, 뿌리부터 각 단어들까지 가는 경로들의 전체 합도 1이 됩니다. 최종적으로 확률 분포를 이루게 되는 것이죠. 이를 수식으로 나타내면 아래와 같습니다.

$$ \sum_{w=1}^{V} p(w | w_{c}) = 1 $$

위 수식은 중심단어 $ w_{c} $ 가 주어졌을 때 Vocabulary 안의 특정 단어 $w$가 올 확률들을 모두 더하면 1이라는 뜻입니다.

이와 같이 binary tree를 이용하여 Softmax를 대체할 수 있으며, 손실함수는 동일하게 cross-entropy를 적용하여

$$ J = -\log p(w | w_{i}) $$

로 정의할 수 있습니다.

논문에 따르면 손실함수의 gradient를 구하는 과정에서의 연산량은 $L(w_{o})$에 비례하며, 최대 $\log$ V 의 연산량을 가지고, 평균적으로는 $\log_{2}$ V 의 연산량을 가지게 된다고 합니다. 기존의 V 만큼의 연산량에 비해서는 획기적으로 줄어들게 됩니다.

원문에서는 [Huffman tree](https://en.wikipedia.org/wiki/Huffman_coding)를 이용하여 계층적 소프트맥스(Hierarchial Softmax)를 구현했습니다. Huffman tree를 이용하면 빈도가 높은 단어일수록 뿌리와 가깝게 생성되므로 실질적인 연산량을 더욱 줄일 수 있을 것 같네요.

## 네거티브 샘플링 (Negative Sampling)





## 마치며



## References
[BEOMSU KIM 님의 블로그](https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/)
[Inhwan Lee 님의 블로그](https://lih0905.github.io/nlp/Word2vec_2/)   

[Word2Vec 논문](https://arxiv.org/pdf/1301.3781.pdf)
[Word2Vec 개선 방법 논문](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
